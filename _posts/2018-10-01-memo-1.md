---
layout: post
title: 'about tensorflow, deep learning : 메모'
author: memo
date: 2018-10-01 00:40
tags: [deeplearning,tensorflow]
image: /files/covers/idea.jpg
---
# 세상은 넓고 배울 건 많다
## <span style="color:deeppink"> 그래서 재미있다 </span>
데이터 분석과 그를 위한 머신러닝 기법에 대해 공부하다 보니 (아직 기본도 통달 한 것 같진 않지만) 딥러닝에 관심이 생겨서 홍콩과기대 김성훈 교수님의 모두의 딥러닝을 듣고있습니다.
데이터 분석 동아리 사람들과 2주에 한번 딥러닝 스터디를 시작하기도 했는데, 교수님의 강의는 그 이해를 더 증진시켜 줄 것입니다.
기존에 해오던, 분석을 해와서 발표하는 기본세션과 달리 딥러닝 스터디는 포스텍에 석사과정을 다니고 계시는 선배가 오셔서 강의를 해주는 방식입니다. 이런기회가 흔치 않음을 알기에 감사하며 배우고 있습니다.

오늘은 아직 완벽하지 않지만 나중에 까먹지 않기 위해 모두의 딥러닝 기본차시를 정리한 부분과, 선배님의 스터디에서 들었던 딥러닝에 대한 기본 소개에 대해 정리해 보겠습니다.
해당 포스팅이 다루는 부분은 아직 손 볼 곳이 정!말!많기 때문에 '잔상'정도로 기록하고, 후에 틀린부분은 고쳐나가고 모르는 부분은 세세하게 다시 공부해 나가려 합니다.

### Tensorflow 기본
-	소스코드, 기여, 라이브러리 단연 일등
-	Data flow graphs를 사용해서 numerical computation을 한다
-	node: 동그라미 (mathematical operations)
-	edge: 줄 (data arrays = tensors)
다운받은 뒤
 `imort tensorflow as tf`
`tf._version_` 으로 버전 확인
-	`.constant`(“적을 것”)
-	`Session()` : 세션을 여는 이유는 계산속도를 위해 효율적으로 활성화 시키기 때문!
-	`sess.run()`
-	`placeholder()`: 그래프 미리 만든 후 실행 시킬 때 값을 주고 싶다면, 그래서 `sess.run`할 때 `feed_dict={a:,b:}` 꼭 필요

### Structure of Tensorflow
- Rank: 차원(리스트 안에~) [개수]
- Shape: [rank수에 따라 숫자 개수] (각 차원 마다의 element개수를 적으면 됨)
- Types:.float32  / .int32
- Axis: shape의 각 차원들 지칭이라 생각 하면 될 듯. 가장 안쪽이 axis가 제일 크며(0,1,2,3…이니까 당연하다) 마지막 축은 axis=-1이렇게도 당근 할 수 있음!

### Tensorflow에서 주의할 점
- matmul: 매트릭스 곱! (a,b)있고 (b,c) 중간에 b가 맞아야 가능!(행렬계산)
- multiply: 그냥 곱!

이 둘을 헷갈릴 경우 전혀 다른 결과가 나올 수 있음 주의 (warning이 안 뜨고 진짜 계산해준다)
(왜? Broadcasting개념 때문 – 조심해서 사용해야 할 개념)

`.reduce_mean()`: interger인지 float인지 잘 보기(?) (정확하게 안 나오고 정수 값 나올 수 있으니까)

## 딥러닝 강의_동아리에서
- CNN : 컴퓨터 비전에서 많이 쓰임!
- 오늘의 자료 강의 이름: CNN for visual recognition
- Top-5 error: 5개의 가장 확률 높은 것 중에서 맞는 것 있으면 맞는 걸로!
- Perceptron만 많이 붙인다고 표현력이 늘어나는 건 아님
표현력이 늘어나려면 activation function(항상 비선형)을 중간에! (ReLU-얘를 많이 씀 에러를 전달 잘하기 때문(?), Sigmoid…)
- Dropout: 40~60%정도로 끄는 확률을 줌. 그러면 꺼지는 노드가 있을 것! => 앙상블 효과
But 단점 -> 같은 성능을 내기까지 학습시간이 오래 걸림 / 근데 일반화 성능이 좋아서 씀
- Iteration을 얼마나 시켰나는 컴퓨터 비전에서 중요한 평가요소가 될 수 있음
- Feature engineering 이제 그 분야는 옛날로.. 이미 그게 뉴럴에서 되기 때문에 옛날엔 filter를 수학적으로 계산해서 했다면 (h[n]) 요즘은 학습시켜서 가중치 줌
- Hard training(hard example): 만약 98개 중에 2개가 진짜 분류가 잘 안되면 2개의 예시들을 여러 개로 해서 더 가중치 주고 해서 학습시킴(boosting 개념 비슷한 맥락에서 이해될 수 있을 것 같다)

### 분류문제에서 쓰이는 것
용도에 맞게 아래의 것들을 쓸 수 있음
- Detection: 찾아서 똥그라미(네모) 쳐주기
- Reception: 그 개체가 뭔지 말해주기 (지문인식 같은거?)
- Segmentation: 모든 픽셀에 대해서 얜 뭔지 말해주기. 분류문제의 끝판왕(자율주행에서 중요)
(youtube: body party segmentation)

### 더 찾아보기
Deep Residual Network
